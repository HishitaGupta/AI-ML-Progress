{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cc7b1d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# PART 1 — FEATURE ENGINEERING (Interview Gold)\n",
    "\n",
    "## 1. What is Feature Engineering?\n",
    "\n",
    "**Best interview answer:**\n",
    "\n",
    "> Feature engineering is the process of transforming raw data into meaningful features that make machine learning models perform better by improving accuracy, reducing noise, and capturing important patterns.\n",
    "\n",
    "**Simple line:**\n",
    "\n",
    "> Better features = Better model performance\n",
    "\n",
    "**Real-life example:**\n",
    "\n",
    "* Raw: `Date = 2025-07-15`\n",
    "* Engineered features: `day`, `month`, `is_weekend`, `holiday_flag`\n",
    "\n",
    "That’s feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why is Feature Engineering important?\n",
    "\n",
    "Say this in interview:\n",
    "\n",
    "> Most model performance comes not from the algorithm, but from the quality of the features. Good feature engineering can significantly increase accuracy even with a simple model.\n",
    "\n",
    "**Benefits:**\n",
    "✅ Improves accuracy\n",
    "✅ Reduces noise\n",
    "✅ Helps model understand data\n",
    "✅ Reduces overfitting\n",
    "✅ Makes patterns visible\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Feature Engineering (You must know)\n",
    "\n",
    "### A. Handling Missing Values\n",
    "\n",
    "Methods:\n",
    "\n",
    "* Mean / Median / Mode\n",
    "* Forward / Backward fill\n",
    "* Predict missing with model\n",
    "\n",
    "**Interview Answer:**\n",
    "\n",
    "> The method depends on the data distribution and context. I usually use median for numerical and mode for categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Handling Categorical Data\n",
    "\n",
    "| Method                 | When to use      |\n",
    "| ---------------------- | ---------------- |\n",
    "| Label Encoding         | Ordinal data     |\n",
    "| One-hot encoding       | Nominal data     |\n",
    "| Target / Mean encoding | High cardinality |\n",
    "| Frequency encoding     | Large categories |\n",
    "\n",
    "**Interview line:**\n",
    "\n",
    "> I use one-hot encoding for nominal data and label encoding for ordinal data.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "Color: Red, Blue, Green\n",
    "One-hot → [1,0,0], [0,1,0], [0,0,1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### C. Feature Scaling (VERY IMPORTANT)\n",
    "\n",
    "Algorithms that require scaling:\n",
    "✅ K-Means\n",
    "✅ SVM\n",
    "✅ KNN\n",
    "✅ Linear/Logistic Regression\n",
    "✅ PCA\n",
    "\n",
    "Methods:\n",
    "\n",
    "1. Standardization (Z-score)\n",
    "2. Min-Max Scaling\n",
    "\n",
    "**Best interview answer:**\n",
    "\n",
    "> Scaling is necessary for distance-based models and gradient-based models to prevent features with large values from dominating the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "### D. Outlier Handling\n",
    "\n",
    "Methods:\n",
    "\n",
    "* IQR method\n",
    "* Z-score\n",
    "* Isolation forest\n",
    "\n",
    "**Interview answer:**\n",
    "\n",
    "> Outliers can distort model learning especially in distance-based models, so identifying and treating them is essential.\n",
    "\n",
    "---\n",
    "\n",
    "### E. Feature Transformation\n",
    "\n",
    "Common transformations:\n",
    "\n",
    "* Log\n",
    "* Square root\n",
    "* Box-Cox\n",
    "* Yeo-Johnson\n",
    "\n",
    "Used when data is skewed.\n",
    "\n",
    "**Interview line:**\n",
    "\n",
    "> Transformations help in making data more normally distributed which helps linear models.\n",
    "\n",
    "---\n",
    "\n",
    "### F. Creating New Features (IMPORTANT)\n",
    "\n",
    "Types:\n",
    "\n",
    "* Polynomial features\n",
    "* Interaction features\n",
    "* Binning\n",
    "* Date-time features\n",
    "* Domain specific\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "[Length, Width] → Area = Length × Width\n",
    "```\n",
    "\n",
    "Say:\n",
    "\n",
    "> Feature creation helps capture hidden patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Feature Engineering Interview Questions\n",
    "\n",
    "### Q1: What feature engineering did you use in your last project?\n",
    "\n",
    "Good structure:\n",
    "\n",
    "1. Missing value handling\n",
    "2. Categorical encoding\n",
    "3. Scaling\n",
    "4. New feature creation\n",
    "5. Outlier treatment\n",
    "\n",
    "Example answer:\n",
    "\n",
    "> In my last project, I handled missing values using median imputation, applied one-hot encoding for categorical features, standardized numeric features, created new time-based features from the date column, and removed outliers using the IQR method.\n",
    "\n",
    "PERFECT.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: When does feature engineering give more benefit than changing algorithm?\n",
    "\n",
    "Answer:\n",
    "\n",
    "> When the model is already strong like XGBoost, better feature quality improves performance more than trying multiple algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "# PART 2 — DIMENSIONALITY REDUCTION (VERY IMPORTANT)\n",
    "\n",
    "## 1. What is Dimensionality Reduction?\n",
    "\n",
    "**Interview ready answer:**\n",
    "\n",
    "> Dimensionality reduction is the process of reducing the number of input features while preserving as much important information as possible.\n",
    "\n",
    "Example:\n",
    "100 features → 20 most important features\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why is it needed?\n",
    "\n",
    "This is a must-answer:\n",
    "\n",
    "✅ Reduces overfitting\n",
    "✅ Improves model performance\n",
    "✅ Removes multicollinearity\n",
    "✅ Faster training\n",
    "✅ Better visualization\n",
    "\n",
    "Say:\n",
    "\n",
    "> High dimensional data suffers from the curse of dimensionality, so we reduce dimensions to improve efficiency and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Dimensionality Reduction\n",
    "\n",
    "### A. Feature Selection\n",
    "\n",
    "Remove unnecessary features\n",
    "\n",
    "Types:\n",
    "• Filter methods (correlation, chi-square)\n",
    "• Wrapper methods (RFE)\n",
    "• Embedded methods (Lasso)\n",
    "\n",
    "### B. Feature Extraction\n",
    "\n",
    "Create new features\n",
    "\n",
    "Examples:\n",
    "• PCA\n",
    "• LDA\n",
    "• t-SNE\n",
    "• UMAP\n",
    "• Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## 4. MOST IMPORTANT: PCA (Principal Component Analysis)\n",
    "\n",
    "You WILL be asked this.\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "> PCA is a dimensionality reduction technique that converts original correlated features into a smaller number of uncorrelated features called principal components that capture maximum variance.\n",
    "\n",
    "**Key points to say:**\n",
    "✅ Reduces dimensions\n",
    "✅ Keeps most important info\n",
    "✅ Components are orthogonal\n",
    "✅ Uses eigenvectors\n",
    "\n",
    "### How PCA works (simple)\n",
    "\n",
    "1. Standardize data\n",
    "2. Find covariance matrix\n",
    "3. Calculate eigenvalues and eigenvectors\n",
    "4. Select top components\n",
    "5. Project data\n",
    "\n",
    "**Interview simplified line:**\n",
    "\n",
    "> PCA finds directions of maximum variance and projects data onto them.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. PCA vs LDA (must know)\n",
    "\n",
    "| PCA          | LDA                  |\n",
    "| ------------ | -------------------- |\n",
    "| Unsupervised | Supervised           |\n",
    "| Max variance | Max class separation |\n",
    "| No labels    | Needs labels         |\n",
    "\n",
    "Say:\n",
    "\n",
    "> PCA focuses on variance, LDA focuses on class separation.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. t-SNE & UMAP\n",
    "\n",
    "Used for visualization (2D/3D)\n",
    "\n",
    "Say in interview:\n",
    "\n",
    "> For visualization of high-dimensional data, I prefer t-SNE or UMAP.\n",
    "\n",
    "---\n",
    "\n",
    "# Highly Asked Dimensionality Reduction Questions\n",
    "\n",
    "### Q1. What is curse of dimensionality?\n",
    "\n",
    "> As dimensions increase, data points become sparse, making models less effective.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Does PCA always improve accuracy?\n",
    "\n",
    "Perfect answer:\n",
    "\n",
    "> No. PCA may remove features that are important for prediction, so sometimes accuracy drops.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. Should PCA be applied before or after scaling?\n",
    "\n",
    "> Always after scaling.\n",
    "\n",
    "Very important.\n",
    "\n",
    "---\n",
    "\n",
    "# REAL INTERVIEW SCENARIO ANSWER\n",
    "\n",
    "**Q: You have 200 features. What do you do?**\n",
    "\n",
    "Say:\n",
    "\n",
    "> I would first do feature selection using correlation and feature importance. If still high dimensional, I would apply PCA to reduce features.\n",
    "\n",
    "This is perfect.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> In most projects, good feature engineering gives more improvement than changing algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91a314",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
