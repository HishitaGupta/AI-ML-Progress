{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d2f057",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 1. What is Machine Learning ?\n",
    "\n",
    "Most people say:\n",
    "\n",
    "> â€œML is learning from dataâ€\n",
    "\n",
    "That is correct but shallow. Letâ€™s go deeper.\n",
    "\n",
    "### Real meaning of ML\n",
    "\n",
    "At its core, **Machine Learning is function finding.**\n",
    "\n",
    "You are trying to find a function `f(x)` such that:\n",
    "\n",
    "[\n",
    "f(x) \\approx y\n",
    "]\n",
    "\n",
    "where:\n",
    "\n",
    "* `x` = input features (data)\n",
    "* `y` = output (what you want to predict)\n",
    "* `f` = the model (unknown function you're discovering)\n",
    "\n",
    "You donâ€™t program `f` manually.\n",
    "The algorithm **learns** `f` from data.\n",
    "\n",
    "### What does \"learning\" really mean?\n",
    "\n",
    "Learning = finding the best **parameters** that minimize error.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "> If size > 2000 and rooms > 3 then price....\n",
    "\n",
    "You are doing:\n",
    "\n",
    "[\n",
    "\\text{Find the best values of } w, b \\text{ such that } y = wx + b\n",
    "]\n",
    "\n",
    "This is why ML is actually:\n",
    "\n",
    "> An optimization + statistics problem disguised as AI.\n",
    "\n",
    "ğŸ’¡ **Machine Learning = Statistics + Optimization + Data**\n",
    "\n",
    "That is the core.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Types of Machine Learning\n",
    "\n",
    "## 2.1 Supervised Learning\n",
    "\n",
    "In supervised learning, **you know the answer while training.**\n",
    "\n",
    "You are saying to the model:\n",
    "\n",
    "> â€œHere is the question AND the answer. Learn the pattern.â€\n",
    "\n",
    "Example:\n",
    "\n",
    "| Size | Rooms | Price |\n",
    "| ---- | ----- | ----- |\n",
    "| 1000 | 2     | 50L   |\n",
    "| 2000 | 3     | 90L   |\n",
    "| 3000 | 4     | 140L  |\n",
    "\n",
    "The model **learns the pattern** between Size/Rooms and Price.\n",
    "\n",
    "Mathematically, you're trying to learn:\n",
    "\n",
    "[\n",
    "y = f(x_1, x_2, x_3, ...)\n",
    "]\n",
    "\n",
    "Two types:\n",
    "\n",
    "### Regression â†’ Output is continuous\n",
    "\n",
    "Examples:\n",
    "\n",
    "* â‚¹50,000\n",
    "* 32.5Â°C\n",
    "* 123.9 kg\n",
    "\n",
    "### Classification â†’ Output is category\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Yes / No\n",
    "* Fraud / Not Fraud\n",
    "* Dog / Cat / Cow\n",
    "\n",
    "**Important insight:**\n",
    "\n",
    "> In supervised learning, the data â€œsupervisesâ€ the model with correct answers.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Unsupervised Learning\n",
    "\n",
    "Here, no one tells the system the answer.\n",
    "\n",
    "You give only data:\n",
    "\n",
    "| height | weight | shopping |\n",
    "| ------ | ------ | -------- |\n",
    "| 170    | 70     | high     |\n",
    "| 165    | 55     | medium   |\n",
    "| 180    | 90     | high     |\n",
    "\n",
    "The model tries to **find structure** inside.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Group similar customers\n",
    "* Detect anomalies\n",
    "* Reduce dimensions\n",
    "\n",
    "Key question solved here:\n",
    "\n",
    "> â€œWhat patterns exist in this data?â€\n",
    "\n",
    "**Very useful in EDA & user segmentation.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Reinforcement Learning\n",
    "\n",
    "Instead of data â†’ output, it's:\n",
    "\n",
    "Agent â†” Environment\n",
    "\n",
    "The agent:\n",
    "\n",
    "* Takes action\n",
    "* Gets reward/punishment\n",
    "* Learns from experience\n",
    "\n",
    "Similar to how humans & animals learn.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Games\n",
    "* Robotics\n",
    "* Self-driving\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Regression \n",
    "\n",
    "Most people just memorize:\n",
    "\n",
    "> Regression predicts numbers\n",
    "\n",
    "That is basic. Now real understanding:\n",
    "\n",
    "### What is regression actually doing?\n",
    "\n",
    "Regression is trying to **draw the best possible function** that fits the data.\n",
    "\n",
    "In **Linear Regression**, that function is a straight line:\n",
    "\n",
    "[\n",
    "y = w_1x_1 + w_2x_2 + w_3x_3 + ... + b\n",
    "]\n",
    "\n",
    "This is actually:\n",
    "\n",
    "[\n",
    "\\hat{y} = \\sum_{i=1}^{n} w_i x_i + b\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* `w` = weights â†’ importance of each feature\n",
    "* `b` = bias/intercept\n",
    "* `x` = features\n",
    "\n",
    "The model is trying to find the **best w and b**.\n",
    "\n",
    "### How does it decide what is \"best\"?\n",
    "\n",
    "By calculating **loss (error)**:\n",
    "\n",
    "[\n",
    "Loss = (Actual - Predicted)^2\n",
    "]\n",
    "\n",
    "For all data points:\n",
    "\n",
    "[\n",
    "MSE = \\frac{1}{n} \\sum (y - \\hat{y})^2\n",
    "]\n",
    "\n",
    "The goal of ML is:\n",
    "\n",
    "[\n",
    "\\text{Minimize Loss}\n",
    "]\n",
    "\n",
    "This is done using **Gradient Descent.**\n",
    "\n",
    "Thatâ€™s why regression is optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Simple Example (Real-life intuition)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "Price = 100 Ã— Size + 5,000\n",
    "\n",
    "You donâ€™t know this rule.\n",
    "\n",
    "Model starts with:\n",
    "Price = 10 Ã— Size + 200\n",
    "\n",
    "It makes big errors.\n",
    "\n",
    "Then it slowly adjusts:\n",
    "\n",
    "10 â†’ 30 â†’ 60 â†’ 90 â†’ 100\n",
    "\n",
    "Each step it improves by reducing error.\n",
    "\n",
    "This is learning.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Regularization (Ridge & Lasso) â€“ Deep Logic\n",
    "\n",
    "### Real Problem: Overfitting\n",
    "\n",
    "Overfitting happens when model becomes:\n",
    "\n",
    "* Too smart\n",
    "* Too complex\n",
    "* Memorizes data\n",
    "\n",
    "Instead of learning pattern, it learns **noise**.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Size | Price |\n",
    "| ---- | ----- |\n",
    "| 1000 | 50    |\n",
    "| 1500 | 70    |\n",
    "| 2000 | 90    |\n",
    "\n",
    "Instead of learning a straight line, model tries to touch every point exactly using zig-zag curve.\n",
    "\n",
    "That looks perfect for training data, but fails on new data.\n",
    "\n",
    "This is **high variance**.\n",
    "\n",
    "---\n",
    "\n",
    "## What Regularization REALLY does\n",
    "\n",
    "Regularization = **punishing complexity**\n",
    "\n",
    "We add a penalty to the loss:\n",
    "\n",
    "### Without Regularization:\n",
    "\n",
    "[\n",
    "Loss = Error\n",
    "]\n",
    "\n",
    "### With Regularization:\n",
    "\n",
    "[\n",
    "Loss = Error + Penalty\n",
    "]\n",
    "\n",
    "Penalty means:\n",
    "\n",
    "> â€œIf your weights are too big, I will punish you.â€\n",
    "\n",
    "This forces them to remain small â†’ simpler model.\n",
    "\n",
    "---\n",
    "\n",
    "## Ridge (L2) â€“ \n",
    "\n",
    "Ridge adds:\n",
    "\n",
    "[\n",
    "\\lambda \\sum w^2\n",
    "]\n",
    "\n",
    "This means:\n",
    "\n",
    "* If weight is large â†’ squared value becomes very big\n",
    "* Model is forced to reduce them\n",
    "\n",
    "Ridge does:\n",
    "\n",
    "* Shrinks weights\n",
    "* Keeps all features\n",
    "* Makes model smooth\n",
    "\n",
    "**Meaning:**\n",
    "\n",
    "> \"All features matter, but not equally strongly.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Lasso (L1) â€“ \n",
    "\n",
    "Lasso adds:\n",
    "\n",
    "[\n",
    "\\lambda \\sum |w|\n",
    "]\n",
    "\n",
    "This penalty makes small weights go **exactly zero**\n",
    "\n",
    "Which means:\n",
    "\n",
    "* Unimportant features = removed\n",
    "* Built-in feature selection\n",
    "\n",
    "Lasso does:\n",
    "\n",
    "* Feature elimination\n",
    "* Dimension reduction\n",
    "* Simpler model\n",
    "\n",
    "If dataset has 100 features:\n",
    "Lasso might reduce to 15 important ones.\n",
    "\n",
    "Thatâ€™s very powerful.\n",
    "\n",
    "---\n",
    "\n",
    "## Simple comparison (for interview)\n",
    "\n",
    "Imagine 5 friends helping you move:\n",
    "\n",
    "* Ridge: everyone helps, but some less\n",
    "* Lasso: some friends are told: \"donâ€™t come\"\n",
    "\n",
    "Thatâ€™s the difference.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Bias & Variance â€“ Deepest Understanding\n",
    "\n",
    "This is one of the most important interview questions.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "You are shooting arrows at a target.\n",
    "\n",
    "## High Bias\n",
    "\n",
    "All arrows miss target in same place.\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Model is consistently wrong\n",
    "* Too simple\n",
    "* Underfitting\n",
    "\n",
    "## High Variance\n",
    "\n",
    "Arrows are all over the place.\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Model is unstable\n",
    "* Overfitting\n",
    "* Memorizes noise\n",
    "\n",
    "### Perfect model\n",
    "\n",
    "Arrows are tightly clustered at center.\n",
    "\n",
    "That is:\n",
    "**Low Bias + Low Variance**\n",
    "\n",
    "---\n",
    "\n",
    "## Why trade-off exists\n",
    "\n",
    "As you make model more complex:\n",
    "\n",
    "Bias â†“\n",
    "Variance â†‘\n",
    "\n",
    "As you make model simpler:\n",
    "\n",
    "Bias â†‘\n",
    "Variance â†“\n",
    "\n",
    "So you must **balance**.\n",
    "\n",
    "Thatâ€™s the trade-off.\n",
    "\n",
    "This is why:\n",
    "\n",
    "* Simple model â†’ underfits\n",
    "* Complex model â†’ overfits\n",
    "* Balanced â†’ generalizes\n",
    "\n",
    "This is ML in a nutshell.\n",
    "\n",
    "---\n",
    "\n",
    "# Interview-Level Summary (Memorize This)\n",
    "\n",
    "You can literally speak this in interviews:\n",
    "\n",
    "> Machine learning is the process of learning a function that maps input features to output targets using data. In regression problems, the model tries to minimize a loss function such as MSE by optimizing its parameters through gradient descent. However, if the model becomes too complex, it may overfit the data, which is controlled using regularization such as Ridge and Lasso. Ridge shrinks coefficients using L2 penalty while Lasso uses L1 penalty and can remove irrelevant features completely. This balance between model complexity and simplicity is captured in the Biasâ€“Variance tradeoff.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1849e01",
   "metadata": {},
   "source": [
    "| Aspect                              | RÂ² Score         | Adjusted RÂ²     |\n",
    "| ----------------------------------- | ---------------- | --------------- |\n",
    "| Considers number of features        | âŒ No             | âœ… Yes           |\n",
    "| Penalizes overfitting               | âŒ No             | âœ… Yes           |\n",
    "| Always increases with more features | âœ… Yes            | âŒ No            |\n",
    "| Best for feature comparison         | âŒ No             | âœ… Yes           |\n",
    "| Used in                             | Basic regression | Model selection |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98257e3",
   "metadata": {},
   "source": [
    "Why Adjusted RÂ² is Important in ML Interviews\n",
    "Interviewer Wants to Hear:\n",
    "\n",
    "â€œRÂ² alone is misleading when comparing models with different numbers of features. Adjusted RÂ² compensates for this by penalizing unnecessary predictors.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e49b17",
   "metadata": {},
   "source": [
    "How to Explain in 30 Seconds (Perfect Interview Answer)\n",
    "\n",
    "â€œRÂ² measures how much variance in the target variable is explained by the model, but it always increases when we add more features, even if they are irrelevant. Adjusted RÂ² solves this problem by penalizing unnecessary features and only increases if a new variable genuinely improves the model. Thatâ€™s why Adjusted RÂ² is preferred for comparing multiple regression models.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eeb421",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
