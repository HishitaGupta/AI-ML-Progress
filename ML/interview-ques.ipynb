{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f78c46e",
   "metadata": {},
   "source": [
    "# DATA + BUSINESS SCENARIO QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "Your model accuracy is 95% in training, but only 63% in production. What is happening and how will you fix it?\n",
    "\n",
    "A food delivery company sees that orders are dropping in one city. As a data scientist, how would you analyze the problem step-by-step?\n",
    "\n",
    "Netflix changed the recommendation algorithm. Watch time dropped by 12%. What would you investigate?\n",
    "\n",
    "Several features in your dataset are highly correlated. What problems can this cause and how will you handle it?\n",
    "\n",
    "You‚Äôre given a dataset with 40% missing values in one column. Will you drop or keep the column? Why?\n",
    "\n",
    "Your company wants to increase ad clicks. What ML approach would you use and what metric will you optimize?\n",
    "\n",
    "Suddenly there is a spike in model errors, but the data pipeline did not change. What might be the cause?\n",
    "\n",
    "You deploy a churn prediction model. After 3 months, its performance drops. Why does this happen?\n",
    "\n",
    "‚úÖ MODELING DECISION QUESTIONS\n",
    "\n",
    "When will you choose Logistic Regression over XGBoost, even if XGBoost performs better?\n",
    "\n",
    "A dataset has 50,000 rows and 10 features vs 500 rows and 200 features. Which dataset is harder to model and why?\n",
    "\n",
    "You are solving fraud detection. Accuracy is 99% but fraud detection rate is low. Is your model good?\n",
    "\n",
    "For a recommendation system, which approach would you use:\n",
    "\n",
    "Content based\n",
    "\n",
    "Collaborative filtering\n",
    "\n",
    "Hybrid ‚Äî and why?\n",
    "\n",
    "If your dataset is small, would you choose Deep Learning? Why or why not?\n",
    "\n",
    "Why might Random Forest perform better than Decision Tree?\n",
    "\n",
    "Why does Gradient Boosting often outperform Random Forest on structured data?\n",
    "\n",
    "‚úÖ EVALUATION + METRIC SCENARIOS\n",
    "\n",
    "Facebook wants to detect fake accounts. Should you use accuracy? Why?\n",
    "\n",
    "In medical cancer detection, what is more important: Precision or Recall?\n",
    "\n",
    "Your AUC is high, but Recall is low. What does this mean?\n",
    "\n",
    "What would be the worst metric to use for highly imbalanced data?\n",
    "\n",
    "How will you decide the threshold in a classification model?\n",
    "\n",
    "‚úÖ EDA REAL LIFE SCENARIOS\n",
    "\n",
    "How do you detect data leakage before training?\n",
    "\n",
    "What if the target variable is extremely skewed?\n",
    "\n",
    "You notice a feature that is almost perfectly correlated with the target. What will you do?\n",
    "\n",
    "How will you deal with outliers in a housing price model?\n",
    "\n",
    "How will you check if features are important or just noise?\n",
    "\n",
    "‚úÖ CLUSTERING + SEGMENTATION SCENARIOS\n",
    "\n",
    "A retail company wants to segment users. Which algorithm and how will you choose K?\n",
    "\n",
    "Data is not spherical shaped. Which clustering is better than K-means?\n",
    "\n",
    "How do you measure success of a clustering model without labels?\n",
    "\n",
    "‚úÖ DEPLOYMENT + PRODUCTION SCENARIOS (VERY IMPRESSIVE IF YOU ANSWER WELL)\n",
    "\n",
    "How do you detect concept drift in production?\n",
    "\n",
    "How often should you retrain your model and why?\n",
    "\n",
    "A model is performing well technically but business KPIs are not improving. Why?\n",
    "\n",
    "Your model is too slow for real-time prediction. What can you do?\n",
    "\n",
    "How do you make a machine learning solution scalable?\n",
    "\n",
    "‚úÖ ADVANCED / TOP-COMPANY THINKING\n",
    "\n",
    "You have 300 features. How will you select the top 20?\n",
    "\n",
    "How do you balance interpretability vs accuracy?\n",
    "\n",
    "Why might a simpler model outperform a complicated one?\n",
    "\n",
    "Would you prefer F1-score or ROC-AUC for email spam? Why?\n",
    "\n",
    "How do you test whether a new ML model is truly better?\n",
    "\n",
    "‚úÖ BONUS: QUICK CHALLENGE (Asked in Interviews)\n",
    "\n",
    "Predict whether a user will click on an ad. Design the full solution.\n",
    "\n",
    "Design an ML system for recommending reels/shorts on Instagram.\n",
    "\n",
    "How would you design price prediction for Uber/Ola?\n",
    "\n",
    "How do you detect fake reviews on Amazon?\n",
    "\n",
    "Design a content moderation ML system.\n",
    "\n",
    "Build a resume screening system. What bias issues can occur?\n",
    "\n",
    "Predict students who will drop out of a course.\n",
    "\n",
    "‚úÖ HARD INTELLIGENCE QUESTIONS\n",
    "\n",
    "If you had unlimited data, would your model be perfect? Why / why not?\n",
    "\n",
    "Why does adding more data sometimes make a model worse?\n",
    "\n",
    "Can unsupervised learning be used for prediction? Explain.\n",
    "\n",
    "How do you know if your dataset represents the real world?\n",
    "\n",
    "What question do you ask before even building an ML model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75584a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2616f6f7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. **Your model accuracy is 95% in training, but only 63% in production. What is happening & how do you fix it?**\n",
    "\n",
    "This is a classic case of **overfitting + data mismatch**.\n",
    "\n",
    "### ‚úÖ What‚Äôs likely happening\n",
    "\n",
    "1. **Overfitting**\n",
    "\n",
    "   * Model memorized training data instead of learning general patterns\n",
    "   * Very high training accuracy but poor real-world performance\n",
    "\n",
    "2. **Data shift / Data drift**\n",
    "\n",
    "   * Production data distribution is different from training data\n",
    "   * Examples:\n",
    "\n",
    "     * New user behavior\n",
    "     * Different customer segment\n",
    "     * Seasonal patterns\n",
    "     * Changed data pipeline logic\n",
    "\n",
    "3. **Data leakage during training**\n",
    "\n",
    "   * Model accidentally had access to future info or target-related features during training\n",
    "\n",
    "4. **Incorrect preprocessing in production**\n",
    "\n",
    "   * Missing scaling/encoding in live pipeline\n",
    "   * Different handling of missing values or categories\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ How I would fix it (Step-by-step in interview)\n",
    "\n",
    "1. **First check for Overfitting**\n",
    "\n",
    "   Compare:\n",
    "\n",
    "   * Training accuracy: 95%\n",
    "   * Validation accuracy: ?\n",
    "   * Test accuracy: ?\n",
    "\n",
    "   If validation is also low ‚Üí overfitting confirmed\n",
    "\n",
    "   ‚úÖ Fix:\n",
    "\n",
    "   * Use **regularization (L1/L2)**\n",
    "   * Reduce model complexity\n",
    "   * Add dropout (for deep models)\n",
    "   * Use proper cross-validation\n",
    "\n",
    "2. **Check for data drift**\n",
    "\n",
    "   I would compare:\n",
    "\n",
    "   * Feature distributions (train vs prod)\n",
    "\n",
    "     * Use: KS-test / PSI / histograms\n",
    "\n",
    "   ‚úÖ Fix:\n",
    "\n",
    "   * Retrain with recent data\n",
    "   * Add drift detection monitoring\n",
    "   * Use adaptive learning / scheduled retraining\n",
    "\n",
    "3. **Verify pipeline consistency**\n",
    "\n",
    "   Ensure:\n",
    "\n",
    "   * Same scaler\n",
    "   * Same encoders\n",
    "   * Same feature engineering\n",
    "   * Same feature order\n",
    "\n",
    "   ‚úÖ Fix:\n",
    "\n",
    "   * Save and reuse pipeline with `Pipeline` or `MLFlow`\n",
    "   * Use versioning\n",
    "\n",
    "4. **Re-evaluate metric**\n",
    "\n",
    "   * Maybe accuracy is misleading because data is imbalanced\n",
    "   * I would check: precision, recall, F1, ROC-AUC\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Orders are dropping in one city ‚Äì How will you analyze?**\n",
    "\n",
    "A business + data breakdown question.\n",
    "\n",
    "### ‚úÖ Step-by-step thinking (interview-ready):\n",
    "\n",
    "1. **Validate if the drop is real**\n",
    "\n",
    "   * Compare WoW / MoM trends\n",
    "   * Compare with nearby cities\n",
    "   * Check data error possibility\n",
    "\n",
    "2. **Segment the data**\n",
    "\n",
    "   I would check:\n",
    "\n",
    "   * New users vs returning users\n",
    "   * Android vs iOS\n",
    "   * Time of day pattern\n",
    "   * Weekend vs weekday\n",
    "   * Areas / zones in city\n",
    "\n",
    "3. **Look for operational factors**\n",
    "\n",
    "   * Delivery time ‚Üë?\n",
    "   * Restaurant partners ‚Üì?\n",
    "   * Stock availability ‚Üì?\n",
    "   * Rider availability ‚Üì?\n",
    "   * App issues / crashes?\n",
    "\n",
    "4. **External factors**\n",
    "\n",
    "   * Weather event?\n",
    "   * Local festival or strikes?\n",
    "   * Competitor offers?\n",
    "   * Government restrictions?\n",
    "\n",
    "5. **Cohort analysis**\n",
    "\n",
    "   * Compare user retention\n",
    "   * Are users ordering less or uninstalling?\n",
    "\n",
    "6. **Run hypothesis testing**\n",
    "\n",
    "   Example hypotheses:\n",
    "\n",
    "   * H1: Increase in delivery time ‚Üí less orders\n",
    "   * H2: Increased price ‚Üí reduced ordering\n",
    "\n",
    "   Validate using correlation & A/B tests\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ My final output as a data scientist\n",
    "\n",
    "I would provide:\n",
    "\n",
    "* Root cause(s)\n",
    "* Key metric change (numbers & graphs)\n",
    "* Recommendation:\n",
    "\n",
    "  * Discount campaign\n",
    "  * Increase riders\n",
    "  * Restaurant onboarding\n",
    "\n",
    "Interviewers love **actionable output**, not just analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Netflix algorithm change caused 12% drop in watch time ‚Äì What to investigate?**\n",
    "\n",
    "Very interview-favorite question.\n",
    "\n",
    "### ‚úÖ What I will check:\n",
    "\n",
    "1. **A/B test results**\n",
    "\n",
    "   * Was the algorithm properly tested?\n",
    "   * How big was the sample size?\n",
    "   * Was result statistically significant?\n",
    "\n",
    "2. **User segments impact**\n",
    "\n",
    "   * Did it affect:\n",
    "\n",
    "     * New users more?\n",
    "     * Old users?\n",
    "     * Specific genres?\n",
    "     * Some regions?\n",
    "\n",
    "3. **Recommendation diversity**\n",
    "\n",
    "   * Was it showing too narrow content?\n",
    "   * Is the user bored?\n",
    "\n",
    "4. **Cold-start problem**\n",
    "\n",
    "   * New users not getting good recommendations?\n",
    "\n",
    "5. **Latency & load times**\n",
    "\n",
    "   * Recommendation generation slow = user leaves\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Possible fixes or experiments I'd suggest\n",
    "\n",
    "* Hybrid model: Collaborative + Content-based\n",
    "* Increase content diversity\n",
    "* Re-run experiment with better personalization\n",
    "* Add manual overrides for top content\n",
    "\n",
    "**As a fresher**, saying *\"I would propose another A/B test\"* is important.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Highly correlated features ‚Äì What problems & how to solve?**\n",
    "\n",
    "### ‚úÖ Problems\n",
    "\n",
    "This causes **MULTICOLLINEARITY** which leads to:\n",
    "\n",
    "* Unstable model coefficients\n",
    "* Hard to interpret results\n",
    "* Overfitting risk\n",
    "* Poor generalization\n",
    "\n",
    "Especially affects: **Linear / Logistic Regression**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ How I handle it\n",
    "\n",
    "1. **Correlation matrix + heatmap**\n",
    "\n",
    "2. **Remove one of the correlated features**\n",
    "\n",
    "3. **Use PCA** for dimensionality reduction\n",
    "\n",
    "4. **Use models not affected much**\n",
    "\n",
    "   * Random Forest\n",
    "   * XGBoost\n",
    "\n",
    "5. **Calculate VIF (Variance Inflation Factor)**\n",
    "\n",
    "   If VIF > 5 or 10 ‚Üí remove that feature\n",
    "\n",
    "‚úÖ In interviews, mention: **VIF + PCA** ‚Üí very good impression\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **40% missing values ‚Äì Drop or keep?**\n",
    "\n",
    "### ‚ùå I won't directly drop.\n",
    "\n",
    "### ‚úÖ I decide based on:\n",
    "\n",
    "1. **Feature importance**\n",
    "\n",
    "   * Is that column important to business?\n",
    "   * Domain knowledge\n",
    "\n",
    "2. **Pattern of missing data**\n",
    "\n",
    "   * Random (MCAR)? ‚Üí safe to impute\n",
    "   * Not random (MNAR)? ‚Üí must be careful\n",
    "\n",
    "### ‚úÖ What I can do:\n",
    "\n",
    "| Method        | When I use it  |\n",
    "| ------------- | -------------- |\n",
    "| Mean/Median   | Numerical      |\n",
    "| Mode          | Categorical    |\n",
    "| Interpolation | Time-series    |\n",
    "| Model-based   | Regression/KNN |\n",
    "\n",
    "If:\n",
    "\n",
    "* Highly important + can be filled ‚Üí keep\n",
    "* Not important + too noisy ‚Üí drop\n",
    "\n",
    "Showing this logical decision matters.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Want to increase ad clicks ‚Äì ML approach & metric?**\n",
    "\n",
    "### ‚úÖ This is a classification + ranking problem\n",
    "\n",
    "I would use:\n",
    "\n",
    "* Logistic Regression\n",
    "* XGBoost\n",
    "* Deep Learning\n",
    "* Recommendation system\n",
    "\n",
    "### ‚úÖ Optimize for:\n",
    "\n",
    "NOT accuracy (because data is imbalanced)\n",
    "\n",
    "I will use:\n",
    "\n",
    "* **CTR (Click Through Rate)**\n",
    "* **Precision / Recall**\n",
    "* **ROC-AUC**\n",
    "* **PR-AUC**\n",
    "* **Log Loss**\n",
    "\n",
    "If the business wants more clicks:\n",
    "\n",
    "‚û°Ô∏è Optimize **Recall** (catch more possible clickers)\n",
    "\n",
    "If they want better quality:\n",
    "\n",
    "‚û°Ô∏è Optimize **Precision**\n",
    "\n",
    "Also use:\n",
    "\n",
    "* Context\n",
    "* User behavior\n",
    "* Time\n",
    "* Location\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Spike in model errors, pipeline didn‚Äôt change ‚Äì Why?**\n",
    "\n",
    "### ‚úÖ Possible causes\n",
    "\n",
    "1. **Data drift**\n",
    "2. **Concept drift**\n",
    "3. **Bad data input (nulls, wrong format)**\n",
    "4. **Edge cases suddenly increased**\n",
    "5. **User behavior change**\n",
    "6. **External events (festival, trend)**\n",
    "\n",
    "I would check:\n",
    "\n",
    "* Input data distribution\n",
    "* Feature statistics\n",
    "* Production logs\n",
    "* Data versioning\n",
    "\n",
    "Áî® ‚Üí Statistical tests + monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Churn model drops after 3 months ‚Äì Why?**\n",
    "\n",
    "This is because of:\n",
    "\n",
    "> ‚úÖ **Model decay + concept drift**\n",
    "\n",
    "Reasons:\n",
    "\n",
    "* Customer behavior changed\n",
    "* Business strategy changed\n",
    "* New competitors\n",
    "* Old data used\n",
    "* Model not retrained\n",
    "* Seasonality\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution\n",
    "\n",
    "* Retrain quarterly or monthly\n",
    "* Use rolling window data\n",
    "* Add new feature (usage trends)\n",
    "* Monitor live performance\n",
    "* Auto re-training pipelines\n",
    "\n",
    "This shows **MLOps understanding**, very valuable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac0260",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. When will you choose **Logistic Regression over XGBoost**, even if XGBoost performs better?\n",
    "\n",
    "Even if XGBoost has slightly higher accuracy, I would choose **Logistic Regression** when:\n",
    "\n",
    "**a) Interpretability is more important than performance**\n",
    "\n",
    "* Logistic Regression gives clear coefficients ‚Üí easy to explain to business/stakeholders\n",
    "* Important in finance, healthcare, legal applications\n",
    "\n",
    "**b) Dataset is small & simple**\n",
    "\n",
    "* XGBoost may overfit\n",
    "* Logistic is more stable\n",
    "\n",
    "**c) Fast & low-cost deployment needed**\n",
    "\n",
    "* Logistic is light, less memory, lower latency\n",
    "\n",
    "**d) Need probability explanation**\n",
    "\n",
    "* Logistic gives directly interpretable probabilities\n",
    "\n",
    "‚úÖ In short:\n",
    "\n",
    "> I choose Logistic Regression when **explainability, simplicity and speed** matter more than tiny performance gain.\n",
    "\n",
    "This shows mature decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 50,000 rows & 10 features **vs** 500 rows & 200 features ‚Äî Which is harder to model and why?\n",
    "\n",
    "‚úÖ **500 rows & 200 features is much harder** to model.\n",
    "\n",
    "Because:\n",
    "\n",
    "1. **Curse of Dimensionality**\n",
    "\n",
    "   * Too many features, too little data\n",
    "   * Distance between points becomes meaningless\n",
    "\n",
    "2. **High risk of overfitting**\n",
    "\n",
    "   * Model will memorize instead of generalizing\n",
    "\n",
    "3. **Noise dominates the signal**\n",
    "\n",
    "4. **More preprocessing needed**\n",
    "\n",
    "   * Feature selection\n",
    "   * PCA\n",
    "   * Regularization\n",
    "\n",
    "Whereas:\n",
    "‚úîÔ∏è 50,000 rows & 10 features = more stable and learnable patterns\n",
    "\n",
    "**In short:**\n",
    "\n",
    "> More data with fewer features is much easier than fewer data with many features.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Fraud detection: 99% accuracy but low fraud detection rate ‚Äî Good model?\n",
    "\n",
    "‚ùå **NO ‚Äî it‚Äôs a BAD model.**\n",
    "\n",
    "Because:\n",
    "\n",
    "Fraud detection is an **imbalanced problem**.\n",
    "\n",
    "Example:\n",
    "\n",
    "* 99% transactions are normal\n",
    "* Only 1% is fraud\n",
    "\n",
    "If model predicts **everything as normal** ‚Üí accuracy is still 99% but **zero fraud detection**\n",
    "\n",
    "üìâ Accuracy is misleading.\n",
    "\n",
    "‚úÖ I would focus on:\n",
    "\n",
    "* Recall (fraud detection rate) ‚Äì most important\n",
    "* Precision\n",
    "* F1-score\n",
    "* ROC-AUC / PR-AUC\n",
    "\n",
    "**In fraud detection: missing a fraud is more expensive than false positives**\n",
    "\n",
    "‚úÖ I‚Äôd also use:\n",
    "\n",
    "* SMOTE\n",
    "* Class weights\n",
    "* Threshold tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recommendation system ‚Äì Content / Collaborative / Hybrid? Why?\n",
    "\n",
    "### ‚úÖ Best answer: **Hybrid system**\n",
    "\n",
    "Because:\n",
    "\n",
    "| Method        | Problem                                  |\n",
    "| ------------- | ---------------------------------------- |\n",
    "| Content-based | Only recommends similar things -> boring |\n",
    "| Collaborative | Cold start problem                       |\n",
    "| Hybrid        | Solves both problems                     |\n",
    "\n",
    "‚úÖ Hybrid uses:\n",
    "\n",
    "* User behavior (collaborative)\n",
    "* Item attributes (content)\n",
    "* Popularity & trends\n",
    "\n",
    "That‚Äôs why Netflix & Spotify use **Hybrid**.\n",
    "\n",
    "üß† Smart line for interview:\n",
    "\n",
    "> Hybrid is more robust, scalable and personalized in real-world systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Small dataset ‚Äî Would you choose Deep Learning?\n",
    "\n",
    "‚ùå Generally **NO**.\n",
    "\n",
    "Because:\n",
    "\n",
    "1. Deep learning needs large data\n",
    "2. High risk of overfitting\n",
    "3. Needs more computing\n",
    "4. Harder to tune and explain\n",
    "\n",
    "‚úÖ Instead I would try:\n",
    "\n",
    "* Logistic / SVM\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "* KNN\n",
    "\n",
    "UNLESS:\n",
    "\n",
    "* I am using **transfer learning**\n",
    "* Or pretrained models\n",
    "\n",
    "**Smart answer:**\n",
    "\n",
    "> Without transfer learning, deep learning is usually not suitable for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why might Random Forest perform better than a Decision Tree?\n",
    "\n",
    "A single decision tree:\n",
    "\n",
    "* Overfits easily\n",
    "* Sensitive to noise\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "* Combines many trees\n",
    "* Uses random sampling\n",
    "* Reduces variance\n",
    "* More generalizable\n",
    "\n",
    "‚úÖ Key interview words:\n",
    "\n",
    "* **Bagging**\n",
    "* **Ensemble**\n",
    "* **Reduces Overfitting**\n",
    "* **Better Stability**\n",
    "\n",
    "**Final line:**\n",
    "\n",
    "> Random Forest reduces model variance by averaging multiple trees, leading to better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why does Gradient Boosting often outperform Random Forest on structured data?\n",
    "\n",
    "Because:\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "* Builds trees independently\n",
    "* Focus: reduce variance\n",
    "\n",
    "Gradient Boosting / XGBoost:\n",
    "\n",
    "* Builds trees sequentially\n",
    "* Each tree fixes previous tree‚Äôs mistakes\n",
    "* Reduces **bias + variance**\n",
    "\n",
    "On structured/tabular data:\n",
    "‚úÖ Boosting captures complex patterns more efficiently\n",
    "\n",
    "‚úÖ Also supports:\n",
    "\n",
    "* Regularization\n",
    "* Learning rate control\n",
    "* Handling missing values\n",
    "\n",
    "**Interview closing line:**\n",
    "\n",
    "> Boosting learns from previous errors step-by-step, which makes it stronger on tabular data compared to independent trees in Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ca54e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ‚úÖ **EVALUATION + METRIC SCENARIOS**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Facebook wants to detect fake accounts. Should you use accuracy? Why?**\n",
    "\n",
    "‚ùå **No, accuracy is a bad metric.**\n",
    "\n",
    "Because:\n",
    "\n",
    "* Fake accounts = very rare ‚Üí **highly imbalanced dataset**\n",
    "* Model predicting ‚Äúall accounts are real‚Äù may get **99% accuracy** but detect **0% fake accounts**\n",
    "\n",
    "Facebook cares about:\n",
    "\n",
    "* Catching fake accounts ‚Üí **Recall**\n",
    "* Avoiding banning real users ‚Üí **Precision**\n",
    "\n",
    "Better metrics:\n",
    "\n",
    "* F1 score\n",
    "* Precision-Recall AUC\n",
    "* Recall@K\n",
    "* ROC-AUC\n",
    "\n",
    "**Perfect interview line:**\n",
    "\n",
    "> \"Accuracy hides failure in imbalanced problems. I would optimize Precision, Recall, and F1 instead.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Cancer detection ‚Äî Precision or Recall?**\n",
    "\n",
    "‚û°Ô∏è **Recall is more important.**\n",
    "\n",
    "Reason:\n",
    "\n",
    "* Missing a cancer patient (False Negative) is far more dangerous than a False Positive.\n",
    "* High Recall = catch maximum cancer cases.\n",
    "\n",
    "But also keep:\n",
    "\n",
    "* Acceptable Precision to avoid too many false alarms.\n",
    "\n",
    "**Simple interview answer:**\n",
    "\n",
    "> \"Recall is critical because we don‚Äôt want to miss sick patients.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **3. AUC is high but Recall is low ‚Äî what does it mean?**\n",
    "\n",
    "This means:\n",
    "\n",
    "1. **The model separates classes well overall**,\n",
    "   but‚Ä¶\n",
    "2. **At the chosen threshold**, the model fails to catch positive cases.\n",
    "\n",
    "Why?\n",
    "\n",
    "* Threshold too high ‚Üí model is too conservative\n",
    "* Class is highly imbalanced\n",
    "* Model favors precision over recall\n",
    "\n",
    "Fix:\n",
    "\n",
    "* Lower classification threshold\n",
    "* Optimize for Recall or F1\n",
    "* Use PR-AUC instead of ROC-AUC\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Worst metric for imbalanced data?**\n",
    "\n",
    "‚ùå **Accuracy** is the worst.\n",
    "\n",
    "Why:\n",
    "\n",
    "* Hides model failure\n",
    "* Misleading when classes are skewed\n",
    "* A dummy model can get high accuracy\n",
    "\n",
    "Better:\n",
    "\n",
    "* Precision, Recall\n",
    "* F1\n",
    "* PR-AUC\n",
    "* Balanced Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## **5. How to decide threshold in classification?**\n",
    "\n",
    "This is an important ML engineering question.\n",
    "\n",
    "I would choose threshold based on:\n",
    "\n",
    "### **1. Business Goal**\n",
    "\n",
    "* Fraud detection ‚Üí maximize Recall\n",
    "* Spam detection ‚Üí maximize Precision\n",
    "* Churn ‚Üí maximize F1\n",
    "\n",
    "### **2. Use ROC curve**\n",
    "\n",
    "* Choose point closest to (0,1)\n",
    "\n",
    "### **3. Use Precision-Recall curve**\n",
    "\n",
    "* Pick best tradeoff for rare class\n",
    "\n",
    "### **4. Cost-based thresholding**\n",
    "\n",
    "* Assign cost to FN and FP\n",
    "* Choose threshold minimizing total cost\n",
    "\n",
    "### **5. Grid search**\n",
    "\n",
    "* Evaluate thresholds 0.1 to 0.9\n",
    "\n",
    "**Interview phrase:**\n",
    "\n",
    "> \"Threshold is not fixed at 0.5; I select it based on the business cost of false positives and false negatives.\"\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **EDA REAL-LIFE SCENARIOS**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. How do you detect data leakage before training?**\n",
    "\n",
    "Data leakage means **model sees information it shouldn‚Äôt**.\n",
    "\n",
    "How I detect it:\n",
    "\n",
    "### **1. Suspiciously high accuracy during validation**\n",
    "\n",
    "* Extremely high performance ‚Üí red flag\n",
    "\n",
    "### **2. Features strongly correlated with target**\n",
    "\n",
    "* Example: ‚Äútotal_bill_paid_after_purchase‚Äù\n",
    "\n",
    "### **3. Time leakage**\n",
    "\n",
    "* Using future information (e.g., using future sales)\n",
    "\n",
    "### **4. Validate pipeline**\n",
    "\n",
    "* Preprocessing must be inside cross-validation\n",
    "  (no scaling before splitting)\n",
    "\n",
    "### **5. Domain checks**\n",
    "\n",
    "* Ask: ‚ÄúDoes this feature exist at prediction time?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Target variable extremely skewed ‚Äî what to do?**\n",
    "\n",
    "If target is heavily skewed (e.g., income, house price):\n",
    "\n",
    "### **1. Apply log transformation**\n",
    "\n",
    "* log(target + 1)\n",
    "* Reduces skew ‚Üí model learns better\n",
    "\n",
    "### **2. Use QuantileTransformer / Box-Cox**\n",
    "\n",
    "### **3. Use robust metrics**\n",
    "\n",
    "* MAE instead of RMSE\n",
    "\n",
    "### **4. Winsorization**\n",
    "\n",
    "* Cap extreme values\n",
    "\n",
    "---\n",
    "\n",
    "## **3. A feature is almost perfectly correlated with target ‚Äî what will you do?**\n",
    "\n",
    "Possibilities:\n",
    "\n",
    "### **1. First check if it is VALID**\n",
    "\n",
    "* If it‚Äôs a legitimate feature ‚Üí keep it\n",
    "  (e.g., previous purchase amount predicts next purchase)\n",
    "\n",
    "### **2. If it's leakage**\n",
    "\n",
    "* Example: ‚ÄúFinal price after discount‚Äù when predicting ‚Äúprice‚Äù\n",
    "* REMOVE it\n",
    "\n",
    "### **3. If it makes model unstable**\n",
    "\n",
    "* Drop it if too predictive & suspicious\n",
    "\n",
    "### **4. If redundancy**\n",
    "\n",
    "* Use PCA or drop one of duplicates.\n",
    "\n",
    "Interview answer:\n",
    "\n",
    "> ‚ÄúI check if correlation is due to leakage or genuine predictive power.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Outliers in housing price model ‚Äî what to do?**\n",
    "\n",
    "Outliers greatly impact regression.\n",
    "\n",
    "### **Options:**\n",
    "\n",
    "#### **1. Log-transform price**\n",
    "\n",
    "* Stabilizes extreme values\n",
    "\n",
    "#### **2. Cap outliers**\n",
    "\n",
    "* Winsorize top 1% / bottom 1%\n",
    "\n",
    "#### **3. Remove extreme luxury properties**\n",
    "\n",
    "* If your target audience is normal houses\n",
    "\n",
    "#### **4. Use robust models**\n",
    "\n",
    "* Random Forest\n",
    "* Median regression (Huber)\n",
    "\n",
    "**Interview-ready answer:**\n",
    "\n",
    "> ‚ÄúOutliers in price cause model instability, so I either transform, cap, or use tree-based models.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **5. How to check if features are important or just noise?**\n",
    "\n",
    "### **1. Feature importance scores**\n",
    "\n",
    "* Random Forest / XGBoost importance\n",
    "* Permutation importance\n",
    "\n",
    "### **2. SHAP values**\n",
    "\n",
    "* Best for interpretability\n",
    "\n",
    "### **3. Statistical tests**\n",
    "\n",
    "* ANOVA\n",
    "* Chi-square\n",
    "* Mutual information\n",
    "\n",
    "### **4. Correlation tests**\n",
    "\n",
    "* Pearson / Spearman\n",
    "\n",
    "### **5. Drop-column importance**\n",
    "\n",
    "* Drop feature ‚Üí retrain ‚Üí see performance drop\n",
    "\n",
    "### **6. Regularization**\n",
    "\n",
    "* Lasso removes noisy features automatically\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942c5a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
